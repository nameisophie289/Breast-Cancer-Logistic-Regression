\section{Introduction}

Machine learning has become a central component in modern data-driven decision systems, with classification tasks forming one of its fundamental applications. Among the wide range of classification algorithms, \textbf{Logistic Regression} remains one of the most widely studied and practically used models due to its probabilistic interpretability, mathematical simplicity, and effectiveness on linearly separable data. Despite its name, logistic regression is in fact a \textit{classification} model that estimates the probability of a sample belonging to a particular class by applying the logistic (sigmoid) function to a linear combination of input features.

The primary objective of this study is to \textbf{derive, implement, and analyze Logistic Regression as a probabilistic linear model} from first principles. The work focuses on bridging theoretical understanding with practical implementation by developing the model entirely from scratch using NumPy, rather than relying on high-level libraries such as \texttt{scikit-learn}. Through this process, the model's mathematical foundations---including the logistic function, cross-entropy loss, and gradient descent optimization---are thoroughly examined and validated experimentally.

To demonstrate the modelâ€™s applicability, the \textit{Breast Cancer Wisconsin} dataset was selected, consisting of 569 samples with 30 numerical features and a binary target representing malignant and benign tumors. Feature standardization was applied to ensure numerical stability during optimization. The model was trained for 1000 iterations, achieving a test accuracy of \textbf{97.4\%}, precision of \textbf{98.6\%}, recall of \textbf{97.2\%}, and an F1-score of \textbf{97.9\%}. The cost function showed consistent convergence from 0.246 to 0.098, confirming the correctness of the gradient descent implementation.

Beyond basic training, several experiments were conducted to analyze the influence of learning rate and regularization strength ($\lambda$) on performance and convergence behavior. The model maintained stable accuracy across different regularization values, with the L2 penalty reducing weight magnitude while preserving classification accuracy. A comparison with the \texttt{scikit-learn} implementation yielded close agreement (prediction match rate of 99.1\%), demonstrating the correctness of the custom implementation. Additionally, a \textbf{5-fold cross-validation} procedure achieved an average accuracy of \textbf{95.4\% $\pm$ 1.7\%}, indicating strong generalization capability.

The outcomes of this study highlight how a theoretically grounded, manually implemented logistic regression model can achieve comparable performance to a standard library implementation. This project provides both mathematical insight and practical experience in connecting the underlying probabilistic model to its numerical realization through optimization and evaluation.